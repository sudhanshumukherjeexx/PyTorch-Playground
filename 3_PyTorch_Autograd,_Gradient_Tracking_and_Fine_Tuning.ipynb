{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJM55zdMtUOMjWNTNlxrz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhanshumukherjeexx/PyTorch-Playground/blob/main/3_PyTorch_Autograd%2C_Gradient_Tracking_and_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Programmed by Sudhanshu Mukherjee\n",
        "* 10-21-2024: Collab Notebook\n",
        "* 10-29-2024: Notebook updated with text"
      ],
      "metadata": {
        "id": "4AvjqLuZe_vF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This notebook is inspired by and credits [PyTorch’s Autograd Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). Some of the code presented here is based on that tutorial. However, the explanations and examples in this notebook are tailored to be more beginner-friendly, with simplified language and additional clarifications to help new learners grasp key concepts like `autograd`, `gradient tracking`, and `fine-tuning` using `torch.no_grad()`."
      ],
      "metadata": {
        "id": "-idgWWjkfyNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward and Backward Propogation"
      ],
      "metadata": {
        "id": "M19i-sDUjD1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In neural networks, forward propagation and backward propagation are key processes used during the training of the network.\n",
        "\n",
        "### 1. Forward Propagation:\n",
        "- Forward propagation is the process of passing input data through the neural network layers to produce an output (also known as predictions).\n",
        "#### How it works:\n",
        "  - The input data is passed into the first layer of the neural network.\n",
        "  - Each neuron in the layer applies a weighted sum of the inputs and then applies an activation function to introduce non-linearity.\n",
        "  - The output from the first layer becomes the input for the next layer.\n",
        "  - This process continues through all layers until the final output layer produces the prediction (for example, a classification or regression output).\n",
        "\n",
        "  **The goal is to compute the predicted output for the given input based on the current state of the network's weights.**\n",
        "\n",
        "### 2. Backward Propagation:\n",
        "- Backward propagation (backpropagation) is the process of adjusting the weights of the neural network by calculating the error (loss) and distributing it back through the network.\n",
        "#### How it works:\n",
        "  - After forward propagation produces an output, the loss function compares the predicted output with the actual target value to compute the error.\n",
        "  - Backpropagation computes the gradient of the loss function with respect to each weight in the network using the chain rule of calculus.\n",
        "  - Starting from the output layer, gradients are propagated back through the network to update the weights of the neurons in all layers.\n",
        "\n",
        "  **The purpose is to minimize the error by adjusting the weights using optimization techniques like gradient descent.**"
      ],
      "metadata": {
        "id": "elYSB3uHhJmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfSwAv6Xfs2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "data = torch.rand(1, 3, 64, 64)\n",
        "labels = torch.rand(1, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H73JHDy7kiKg",
        "outputId": "ad3e2a0a-ec6b-464a-c228-ae4cd1addeee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 126MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The below code represents forward pass"
      ],
      "metadata": {
        "id": "bD1zp4Comabt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model(data)"
      ],
      "metadata": {
        "id": "63TTv3xImU_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now, we compare the model's prediction with the actual answer (called labels) to figure out how wrong the prediction is. It is also called the error (or loss).\n",
        "\n",
        "- Next, we need to tell the model how to adjust weights to improve. This process is called **backward propagation** (backpropagation).\n",
        "\n",
        "- We start backpropagation by calling `.backward()` on the error. This tells the system to automatically calculate how much each part of the model contributed to the error. It does this using a feature called **autograd**, which stores these values (called gradients) for each part of the model in its `.grad` attribute.\n",
        "\n"
      ],
      "metadata": {
        "id": "b-j86CaZmo_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = (prediction - labels).sum()\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "N4oOKsEQmQvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we set up an **optimizer**, that helps the model learn by adjusting its weights.\n",
        "- We use an optimizer called **SGD** (Stochastic Gradient Descent) with a learning rate of **0.01** and a momentum of **0.9**.\n",
        "- The learning rate controls how big the updates are when the model adjusts its weights, and momentum helps the model avoid getting stuck in its learning.\n",
        "- Optimizer keeps track of all the model's parameters so it can update them during training."
      ],
      "metadata": {
        "id": "aons-w0_nyGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
      ],
      "metadata": {
        "id": "aD1spMfknlxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- we use the optimizer to update the model’s weights by calling .step(). This starts the gradient descent process.\n",
        "- The optimizer looks at the gradients (the values stored in .grad) and adjusts each setting based on those values. This helps the model improve its predictions by reducing the error."
      ],
      "metadata": {
        "id": "4sYNySW-oujX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.step()"
      ],
      "metadata": {
        "id": "eHkuJeygokLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we have everything ready to start training a neural network.\n",
        "- Now we will discuss how autograd(the automatic gradient calculation) works."
      ],
      "metadata": {
        "id": "8Zyesh37pE-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differentiation in Autograd\n",
        "\n",
        "- Let’s see how autograd works to collect gradients (the values needed to update our model's settings)\n",
        "- We start by creating two tensors, `a` and `b`, with `requires_grad=True`.\n",
        "- This tells autograd that it needs to keep track of all operations involving these tensors so it can calculate the gradients when we need them."
      ],
      "metadata": {
        "id": "6f4n_d0gpg4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([7., 5.], requires_grad=True)"
      ],
      "metadata": {
        "id": "IJGdYmpvo7fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we crete another tensor `S` from `a` and `b`\n",
        "$$S={a^9} − {2b^2}$$"
      ],
      "metadata": {
        "id": "JGN7GJTqrCjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = a**9 - 2*b**2"
      ],
      "metadata": {
        "id": "l88GEglRqpIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's say `a` and `b` are the parameters of a neural network, and `S` is the error or loss that tells us how wrong the network's predictions are.\n",
        "\n",
        "- In neural network training, we need to calculate how changing the parameters `a` and `b` affects the error. These changes are called gradients. For example:\n",
        "\n",
        "- The gradient of `S` with respect to `a` is $$\\frac{∂S}{∂a} = {9a^8}$$ that shows how much the error changes if we adjust a.\n",
        "\n",
        "- The gradient of `S` with respect to `b` is $$\\frac{∂S}{∂b} = {-4b}$$ which shows how much the error changes if we adjust b.\n",
        "\n",
        "- When we call `.backward()` on `S`, autograd automatically calculates these gradients for each parameter and stores them in the `.grad` attribute of `a` and `b`.\n",
        "\n",
        "- Since `S` is a vector, we need to provide a gradient argument when calling `S.backward()`. This argument is a tensor of the same shape as `S`, representing the gradient of `S` with respect to itself is 1(Change of something with respect to itself is always 1)"
      ],
      "metadata": {
        "id": "fZI_eF_Gs2T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "S.backward(gradient=external_grad)"
      ],
      "metadata": {
        "id": "hySGpjy1sDTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Gradients are now deposited at `a.grad` and `b.grad`"
      ],
      "metadata": {
        "id": "MVqrG-juxiwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(9*a**8 == a.grad)\n",
        "print(-4*b == b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9DSQgd7xb33",
        "outputId": "5198ed31-511a-45d4-babf-7476ad59b914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Directed Acyclic Graph(DAG)\n",
        "\n",
        "_**Note:**_\n",
        "\n",
        "**In PyTorch, DAGs (Directed Acyclic Graphs) are dynamic. This means that the graph, which tracks how the model's calculations are done, is built from scratch every time you call `.backward()`. Once the gradients are calculated, the graph is thrown away, and a new one is created during the next forward pass.**\n",
        "\n",
        "**This is important because it allows you to use control flow in your model. For example, you can have `if` statements or loops that change the shape, size, or operations of your neural network on each iteration if needed. PyTorch will still be able to handle it because it builds a fresh graph every time, based on whatever operations happen in that iteration.**"
      ],
      "metadata": {
        "id": "incVBjTyyd9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "1. In PyTorch, autograd keeps track of all the operations on tensors that have their `requires_grad` flag set to `True`. This means that PyTorch will automatically calculate gradients for those tensors when needed.\n",
        "\n",
        "2. However, if you have a tensor that doesn’t need gradients (for example, if it's just an input that you don’t want to change), you can set `requires_grad=False` to exclude it from gradient calculations. This way, PyTorch won’t track or calculate gradients for it.\n",
        "\n",
        "3. Even if you have an operation where only one of the input tensors has `requires_grad=True`, the output of that operation will still require gradients. This is because autograd will continue to track the entire computation to make sure it can compute the gradients for the part that does need them."
      ],
      "metadata": {
        "id": "rIGO4N7eyooa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 5)\n",
        "y = torch.rand(5, 5)\n",
        "z = torch.rand((5, 5), requires_grad=True)\n",
        "\n",
        "a = x + y\n",
        "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
        "b = x + z\n",
        "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94whIdy8x64I",
        "outputId": "1d37c9c7-3a1e-433e-c73d-070c2714f236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does `a` require gradients?: False\n",
            "Does `b` require gradients?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In a neural network (NN), **frozen parameters** are weights that don’t need to compute gradients. Freezing parameters means you stop autograd from tracking changes to them, which can make training faster because fewer computations are needed.\n",
        "\n",
        "- This is helpful when you’re fine-tuning a model. In fine-tuning, we often use a pre-trained model (a model already trained on a large dataset) and keep most of the model unchanged (frozen). We only update the last few layers, like the classifier, to adapt the model to make predictions on a new task or with new labels."
      ],
      "metadata": {
        "id": "8mt7tA3t1fRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# freeze all model parameters\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "l79CMLfK0blK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let’s say we have a new dataset with 10 labels, and we want to fine-tune the ResNet18 model on this dataset.\n",
        "- In ResNet18, the classifier that makes predictions is the last layer, called `model.fc`.\n",
        "- To fine-tune the model, we can replace this last layer with a new Linear layer that has 10 output units (one for each label). This new layer is unfrozen by default, meaning it will be trained and its parameters will be updated during the fine-tuning process."
      ],
      "metadata": {
        "id": "GbirYw37244b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = nn.Linear(512, 10)"
      ],
      "metadata": {
        "id": "LIP9hyaX2jN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now, in our modified ResNet18 model, all the parameters are frozen, meaning they won’t be updated during training. The only part of the model that can learn and update is the new `model.fc` layer that we added.\n",
        "\n",
        "- This means that the **only parameters that compute gradients** (the ones that get adjusted during training) are the `weights` and `bias` of the `model.fc` layer. All other layers in the model stay the same as they were in the pre-trained version.\n",
        "\n",
        "- So, during training, the model will focus on fine-tuning just this final classifier layer to adapt to the new dataset with `10` labels."
      ],
      "metadata": {
        "id": "I_0wX_kc3tig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
      ],
      "metadata": {
        "id": "INjSOOSS3iiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Even though we tell the optimizer about all the parameters in the model, the only ones that are actually computing gradients and getting updated during gradient descent are the weights and bias of the new classifier `model.fc`. All the other layers are frozen and not being updated.\n",
        "\n",
        "- One can also use a special feature in PyTorch called `torch.no_grad()` to temporarily turn off gradient tracking for any operation. This is useful when you don’t want PyTorch to calculate gradients for certain parts of your model (like during evaluation or inference).\n",
        "\n",
        "Here’s an example of how you can use `torch.no_grad()`:"
      ],
      "metadata": {
        "id": "Xrj3ZeRi5Ogw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.randn(1, 3, 224, 224) # image with RGB\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"\\nBefore running anything\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} grad: {param.grad}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  # won't track gradients\n",
        "  output = model(input_data)\n",
        "\n",
        "\n",
        "print(\"\\n\\nAfter running inside torch.no_grad():\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} grad: {param.grad}\")\n",
        "\n",
        "\n",
        "print(f'\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W2YpJ954qzH",
        "outputId": "ac538d11-8867-4edb-ebdc-df85a4ec54dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Before running anything\n",
            "conv1.weight grad: None\n",
            "bn1.weight grad: None\n",
            "bn1.bias grad: None\n",
            "layer1.0.conv1.weight grad: None\n",
            "layer1.0.bn1.weight grad: None\n",
            "layer1.0.bn1.bias grad: None\n",
            "layer1.0.conv2.weight grad: None\n",
            "layer1.0.bn2.weight grad: None\n",
            "layer1.0.bn2.bias grad: None\n",
            "layer1.1.conv1.weight grad: None\n",
            "layer1.1.bn1.weight grad: None\n",
            "layer1.1.bn1.bias grad: None\n",
            "layer1.1.conv2.weight grad: None\n",
            "layer1.1.bn2.weight grad: None\n",
            "layer1.1.bn2.bias grad: None\n",
            "layer2.0.conv1.weight grad: None\n",
            "layer2.0.bn1.weight grad: None\n",
            "layer2.0.bn1.bias grad: None\n",
            "layer2.0.conv2.weight grad: None\n",
            "layer2.0.bn2.weight grad: None\n",
            "layer2.0.bn2.bias grad: None\n",
            "layer2.0.downsample.0.weight grad: None\n",
            "layer2.0.downsample.1.weight grad: None\n",
            "layer2.0.downsample.1.bias grad: None\n",
            "layer2.1.conv1.weight grad: None\n",
            "layer2.1.bn1.weight grad: None\n",
            "layer2.1.bn1.bias grad: None\n",
            "layer2.1.conv2.weight grad: None\n",
            "layer2.1.bn2.weight grad: None\n",
            "layer2.1.bn2.bias grad: None\n",
            "layer3.0.conv1.weight grad: None\n",
            "layer3.0.bn1.weight grad: None\n",
            "layer3.0.bn1.bias grad: None\n",
            "layer3.0.conv2.weight grad: None\n",
            "layer3.0.bn2.weight grad: None\n",
            "layer3.0.bn2.bias grad: None\n",
            "layer3.0.downsample.0.weight grad: None\n",
            "layer3.0.downsample.1.weight grad: None\n",
            "layer3.0.downsample.1.bias grad: None\n",
            "layer3.1.conv1.weight grad: None\n",
            "layer3.1.bn1.weight grad: None\n",
            "layer3.1.bn1.bias grad: None\n",
            "layer3.1.conv2.weight grad: None\n",
            "layer3.1.bn2.weight grad: None\n",
            "layer3.1.bn2.bias grad: None\n",
            "layer4.0.conv1.weight grad: None\n",
            "layer4.0.bn1.weight grad: None\n",
            "layer4.0.bn1.bias grad: None\n",
            "layer4.0.conv2.weight grad: None\n",
            "layer4.0.bn2.weight grad: None\n",
            "layer4.0.bn2.bias grad: None\n",
            "layer4.0.downsample.0.weight grad: None\n",
            "layer4.0.downsample.1.weight grad: None\n",
            "layer4.0.downsample.1.bias grad: None\n",
            "layer4.1.conv1.weight grad: None\n",
            "layer4.1.bn1.weight grad: None\n",
            "layer4.1.bn1.bias grad: None\n",
            "layer4.1.conv2.weight grad: None\n",
            "layer4.1.bn2.weight grad: None\n",
            "layer4.1.bn2.bias grad: None\n",
            "fc.weight grad: None\n",
            "fc.bias grad: None\n",
            "\n",
            "\n",
            "After running inside torch.no_grad():\n",
            "conv1.weight grad: None\n",
            "bn1.weight grad: None\n",
            "bn1.bias grad: None\n",
            "layer1.0.conv1.weight grad: None\n",
            "layer1.0.bn1.weight grad: None\n",
            "layer1.0.bn1.bias grad: None\n",
            "layer1.0.conv2.weight grad: None\n",
            "layer1.0.bn2.weight grad: None\n",
            "layer1.0.bn2.bias grad: None\n",
            "layer1.1.conv1.weight grad: None\n",
            "layer1.1.bn1.weight grad: None\n",
            "layer1.1.bn1.bias grad: None\n",
            "layer1.1.conv2.weight grad: None\n",
            "layer1.1.bn2.weight grad: None\n",
            "layer1.1.bn2.bias grad: None\n",
            "layer2.0.conv1.weight grad: None\n",
            "layer2.0.bn1.weight grad: None\n",
            "layer2.0.bn1.bias grad: None\n",
            "layer2.0.conv2.weight grad: None\n",
            "layer2.0.bn2.weight grad: None\n",
            "layer2.0.bn2.bias grad: None\n",
            "layer2.0.downsample.0.weight grad: None\n",
            "layer2.0.downsample.1.weight grad: None\n",
            "layer2.0.downsample.1.bias grad: None\n",
            "layer2.1.conv1.weight grad: None\n",
            "layer2.1.bn1.weight grad: None\n",
            "layer2.1.bn1.bias grad: None\n",
            "layer2.1.conv2.weight grad: None\n",
            "layer2.1.bn2.weight grad: None\n",
            "layer2.1.bn2.bias grad: None\n",
            "layer3.0.conv1.weight grad: None\n",
            "layer3.0.bn1.weight grad: None\n",
            "layer3.0.bn1.bias grad: None\n",
            "layer3.0.conv2.weight grad: None\n",
            "layer3.0.bn2.weight grad: None\n",
            "layer3.0.bn2.bias grad: None\n",
            "layer3.0.downsample.0.weight grad: None\n",
            "layer3.0.downsample.1.weight grad: None\n",
            "layer3.0.downsample.1.bias grad: None\n",
            "layer3.1.conv1.weight grad: None\n",
            "layer3.1.bn1.weight grad: None\n",
            "layer3.1.bn1.bias grad: None\n",
            "layer3.1.conv2.weight grad: None\n",
            "layer3.1.bn2.weight grad: None\n",
            "layer3.1.bn2.bias grad: None\n",
            "layer4.0.conv1.weight grad: None\n",
            "layer4.0.bn1.weight grad: None\n",
            "layer4.0.bn1.bias grad: None\n",
            "layer4.0.conv2.weight grad: None\n",
            "layer4.0.bn2.weight grad: None\n",
            "layer4.0.bn2.bias grad: None\n",
            "layer4.0.downsample.0.weight grad: None\n",
            "layer4.0.downsample.1.weight grad: None\n",
            "layer4.0.downsample.1.bias grad: None\n",
            "layer4.1.conv1.weight grad: None\n",
            "layer4.1.bn1.weight grad: None\n",
            "layer4.1.bn1.bias grad: None\n",
            "layer4.1.conv2.weight grad: None\n",
            "layer4.1.bn2.weight grad: None\n",
            "layer4.1.bn2.bias grad: None\n",
            "fc.weight grad: None\n",
            "fc.bias grad: None\n",
            "\n",
            "tensor([[-0.5258, -0.4313,  0.0989,  0.5699, -0.7160,  0.3093,  0.5749,  0.0204,\n",
            "         -1.0036,  0.1387]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VvbjUIwk5uK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}